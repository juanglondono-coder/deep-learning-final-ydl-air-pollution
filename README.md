# Trabajo final ‚Äì Deep Learning: Predicci√≥n de NMHC (YDL Air Pollution)

Este repositorio contiene el trabajo final del curso **Deep Learning** (M√≥dulo 4‚Äì5),
basado en la competici√≥n de Kaggle **YDL Air Pollution**.  
El objetivo principal es **predecir la concentraci√≥n horaria de NMHC(GT)** a partir
de variables de calidad del aire y meteorol√≥gicas mediante modelos baseline y
modelos de deep learning (LSTM y GRU).

---

## 1. Video de presentaci√≥n

üëâ **Video en YouTube:** https://youtu.be/5Whr8Q-ZlTc

---

## 2. Datos

Los datos provienen de la competici√≥n:

> **YDL Air Pollution** ‚Äì Kaggle  
> ((https://www.kaggle.com/competitions/ydl-air-pollution/data?select=ridge.csv))

Archivos necesarios:

- `train.csv`  
- `test.csv`  
- `sample.csv`

**Instrucciones:**

1. Descarga los tres archivos desde la p√°gina de la competici√≥n.
2. Col√≥calos en la **ra√≠z del repositorio** (mismo nivel que los notebooks).
3. Al ejecutar en Google Colab, aseg√∫rate de que esos archivos est√©n disponibles en el
   directorio de trabajo (subi√©ndolos al runtime o montando Google Drive).

---

## 3. Estructura del repositorio

La estructura sigue la recomendaci√≥n oficial del curso:

- `01_exploracion_datos.ipynb`  
  - Carga de `train.csv`.  
  - An√°lisis exploratorio: estad√≠sticas descriptivas, distribuci√≥n de variables, detecci√≥n
    de valores inv√°lidos (`-200`), comportamiento temporal del NMHC(GT).

- `02_preprocesado.ipynb`  
  - Conversi√≥n de `Datetime` a tipo fecha-hora y orden temporal.  
  - Reemplazo de `-200` por `NaN` en gases (`CO(GT)`, `NMHC(GT)`, `NOx(GT)`, `NO2(GT)`).  
  - Eliminaci√≥n de filas sin valor de `NMHC(GT)`.  
  - Imputaci√≥n de valores faltantes (mediana) y escalado con `StandardScaler`.  
  - Construcci√≥n de:
    - Vista tabular (`X_tab`, `y_tab`) para modelos baseline.
    - Vista secuencial (`X_seq`, `y_seq`) mediante ventanas deslizantes de 24 horas
      para modelos LSTM/GRU (predicci√≥n 1 paso adelante).

- `03_modelo_baseline.ipynb`  
  - Entrenamiento de modelos cl√°sicos de regresi√≥n sobre la vista tabular:
    - Regresi√≥n lineal m√∫ltiple.  
    - Random Forest Regressor.  
  - Evaluaci√≥n con **MAE** y **RMSE** en el conjunto de test interno.  
  - Tabla de m√©tricas `baseline_metrics.csv` en `results/`.

- `04_modelo_deep_learning.ipynb`  
  - Modelos recurrentes sobre la vista secuencial:
    - LSTM (target en escala original y con `log(1 + NMHC(GT))`).  
    - GRU (target con `log(1 + NMHC(GT))`).  
  - Uso de `EarlyStopping` y divisi√≥n temporal en train/val/test.  
  - C√°lculo de **MAE** y **RMSE** en escala real (tras deshacer el log).  
  - Gr√°ficas de:
    - Curvas de entrenamiento (loss vs val_loss).  
    - Real vs predicho (scatter).  
    - Serie temporal real vs predicha en el conjunto de test.  
  - El modelo final seleccionado es **GRU_log_target**, que obtiene las mejores
    m√©tricas de MAE y RMSE.  
  - Tabla de m√©tricas `dl_metrics.csv` en `results/`.

- `05_submission_kaggle.ipynb` *(opcional pero recomendado)*  
  - Reentrenamiento del modelo **GRU_log_target** usando todas las ventanas de train.  
  - Aplicaci√≥n del mismo preprocesado a `test.csv`.  
  - Construcci√≥n de ventanas de test y predicci√≥n de NMHC(GT).  
  - Creaci√≥n del archivo `submission_gru_log_target.csv` en `submissions/`
    siguiendo el formato de `sample_submission.csv`.

- `results/`  
  - `baseline_metrics.csv`: m√©tricas de los modelos baseline.  
  - `dl_metrics.csv`: m√©tricas de los modelos LSTM/GRU.

- `submissions/`  
  - `submission_gru_log_target.csv`: archivo listo para subir a Kaggle.

- `INFORME_PROYECTO.PDF`  
  - Informe final (‚âà5‚Äì10 p√°ginas) con:
    - Descripci√≥n detallada del problema.  
    - Preprocesado y construcci√≥n de ventanas.  
    - Arquitectura de los modelos (baseline y deep learning).  
    - Resultados cuantitativos.  
    - Discusi√≥n.

---

## 4. C√≥mo ejecutar los notebooks en Google Colab

1. Abre el notebook desde GitHub y selecciona **‚ÄúOpen in Colab‚Äù**  
   (o sube el `.ipynb` manualmente a Colab).
2. Sube `train.csv`, `test.csv` y `sample_submission.csv` al entorno de Colab  
   o monta Google Drive de forma que queden en el directorio de trabajo.
3. Ejecuta las celdas **en orden** de arriba abajo:
   - Primero `01_exploracion_datos`.  
   - Luego `02_preprocesado`.  
   - Despu√©s `03_modelo_baseline`.  
   - Finalmente `04_modelo_deep_learning` (y `05_submission_kaggle` si se usa).
4. Las m√©tricas finales se guardan en la carpeta `results/` y, en su caso,
   el archivo de submission en `submissions/`.

---

## 5. Modelo final y resultados (resumen)

Tras comparar varios modelos:

- Baselines: Regresi√≥n lineal y Random Forest.  
- Deep learning: LSTM y GRU sobre ventanas de 24 horas, con y sin
  transformaci√≥n logar√≠tmica del objetivo.

El modelo que obtiene el mejor compromiso entre MAE y RMSE en el conjunto de test
es una **GRU con target transformado** `log(1 + NMHC(GT))` (`GRU_log_target`),
entrenada con p√©rdida MSE y evaluada en la escala real de NMHC(GT).

Los detalles num√©ricos y las gr√°ficas de ajuste se describen en
`INFORME_PROYECTO.PDF` y en el notebook `04_modelo_deep_learning.ipynb`.

---

## 6. Dependencias principales

Las dependencias est√°ndar usadas en los notebooks son:

- Python 3.x  
- `pandas`, `numpy`, `matplotlib`, `seaborn`  
- `scikit-learn`  
- `tensorflow` / `keras`

En Google Colab estas librer√≠as vienen preinstaladas (solo puede ser necesario
actualizar TensorFlow en algunos casos).

---
